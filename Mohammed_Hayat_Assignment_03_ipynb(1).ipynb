{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayatk190/Mohammed_Hayat_INFO5731_spring2025.ipynb/blob/main/Mohammed_Hayat_Assignment_03_ipynb(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from **scratch instead of using any pre-existing libraries** to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3) and (N=2).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the formula  count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative\n",
        "probabilities of each review in terms of other reviews (abstracts, or tweets) by using the formula  frequency (noun phrase) / max frequency (noun phrase) on the whole dataset.\n",
        "\n",
        "Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041b259a-24b2-44b4-b3f6-95177c752eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigrams:\n",
            "machine learning: 2146\n",
            "of the: 770\n",
            "in the: 402\n",
            "of machine: 334\n",
            "in this: 258\n",
            "deep learning: 255\n",
            "learning algorithms: 252\n",
            "can be: 244\n",
            "learning and: 239\n",
            "to the: 233\n",
            "\n",
            "Top 10 Trigrams:\n",
            "of machine learning: 332\n",
            "in machine learning: 169\n",
            "machine learning algorithms: 155\n",
            "machine learning and: 146\n",
            "in this paper: 116\n",
            "state of the: 111\n",
            "of the art: 106\n",
            "support vector machines: 105\n",
            "machine learning models: 102\n",
            "as well as: 100\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import string\n",
        "\n",
        "# Preprocessing function: remove punctuation, convert to lowercase,\n",
        "# and replace punctuation with spaces.\n",
        "def preprocess_text(text):\n",
        "    processed = ''\n",
        "    for char in text.lower():\n",
        "        if char in string.ascii_lowercase or char.isdigit() or char.isspace():\n",
        "            processed += char\n",
        "        else:\n",
        "            processed += ' '  # Replace punctuation with a space\n",
        "    return processed\n",
        "\n",
        "# Read CSV file using the built-in csv module.\n",
        "filename = 'semantic_scholar_papers.csv'  # Change if needed\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # Skip the header row\n",
        "\n",
        "    # Identify the indices for \"title\" and \"abstract\" columns.\n",
        "    col_idx_title = header.index('title')\n",
        "    col_idx_abstract = header.index('abstract')\n",
        "\n",
        "    # Collect all title and abstract text into a list.\n",
        "    for row in reader:\n",
        "        title = row[col_idx_title]\n",
        "        abstract = row[col_idx_abstract]\n",
        "        if title:    # Check if not empty\n",
        "            all_texts.append(title)\n",
        "        if abstract: # Check if not empty\n",
        "            all_texts.append(abstract)\n",
        "\n",
        "# Combine all text into one string.\n",
        "combined_text = ' '.join(all_texts)\n",
        "\n",
        "# Preprocess the combined text (remove punctuation, lowercase).\n",
        "processed_text = preprocess_text(combined_text)\n",
        "\n",
        "# Split processed text into words.\n",
        "words = processed_text.split()\n",
        "\n",
        "# Generate bigrams (N=2).\n",
        "bigram_counts = {}\n",
        "for i in range(len(words) - 1):\n",
        "    bigram = words[i] + ' ' + words[i+1]\n",
        "    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "# Generate trigrams (N=3).\n",
        "trigram_counts = {}\n",
        "for i in range(len(words) - 2):\n",
        "    trigram = words[i] + ' ' + words[i+1] + ' ' + words[i+2]\n",
        "    trigram_counts[trigram] = trigram_counts.get(trigram, 0) + 1\n",
        "\n",
        "# Sort bigrams and trigrams by frequency in descending order.\n",
        "sorted_bigrams = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_trigrams = sorted(trigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display top 10 bigrams.\n",
        "print(\"Top 10 Bigrams:\")\n",
        "for bigram, freq in sorted_bigrams[:10]:\n",
        "    print(f\"{bigram}: {freq}\")\n",
        "\n",
        "# Display top 10 trigrams.\n",
        "print(\"\\nTop 10 Trigrams:\")\n",
        "for trigram, freq in sorted_trigrams[:10]:\n",
        "    print(f\"{trigram}: {freq}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 (2)"
      ],
      "metadata": {
        "id": "yYCJhi2zmPW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import string\n",
        "\n",
        "# Preprocessing function: remove punctuation, convert to lowercase,\n",
        "# and replace punctuation with spaces.\n",
        "def preprocess_text(text):\n",
        "    processed = ''\n",
        "    for char in text.lower():\n",
        "        if char in string.ascii_lowercase or char.isdigit() or char.isspace():\n",
        "            processed += char\n",
        "        else:\n",
        "            processed += ' '  # Replace punctuation with a space\n",
        "    return processed\n",
        "\n",
        "filename = 'semantic_scholar_papers.csv'  # Update path/filename as needed\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # Skip the header row\n",
        "\n",
        "    # Identify the indices for \"title\" and \"abstract\" columns.\n",
        "    col_idx_title = header.index('title')\n",
        "    col_idx_abstract = header.index('abstract')\n",
        "\n",
        "    # Collect all title and abstract text into a list.\n",
        "    for row in reader:\n",
        "        title = row[col_idx_title]\n",
        "        abstract = row[col_idx_abstract]\n",
        "        if title:    # Check if not empty\n",
        "            all_texts.append(title)\n",
        "        if abstract: # Check if not empty\n",
        "            all_texts.append(abstract)\n",
        "\n",
        "# Combine all text into one string\n",
        "combined_text = ' '.join(all_texts)\n",
        "\n",
        "# Preprocess the combined text (remove punctuation, lowercase)\n",
        "processed_text = preprocess_text(combined_text)\n",
        "\n",
        "# Split processed text into a list of words\n",
        "words = processed_text.split()\n",
        "\n",
        "# Count the occurrences of all single words\n",
        "word_counts = {}\n",
        "for w in words:\n",
        "    word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "# Count all bigrams\n",
        "bigram_counts = {}\n",
        "for i in range(len(words) - 1):\n",
        "    bigram = words[i] + ' ' + words[i+1]\n",
        "    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "# Calculate bigram probabilities: count(w1, w2) / count(w1)\n",
        "bigram_probabilities = {}\n",
        "for bigram, count_bi in bigram_counts.items():\n",
        "    w1, w2 = bigram.split()\n",
        "    # Avoid division-by-zero; in normal usage word_counts[w1] should never be zero\n",
        "    prob = count_bi / word_counts[w1]\n",
        "    bigram_probabilities[bigram] = prob\n",
        "\n",
        "# Sort bigrams by descending probability\n",
        "sorted_bigrams_by_prob = sorted(\n",
        "    bigram_probabilities.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# Display top 10 bigrams by probability\n",
        "print(\"Top 10 Bigrams by Probability:\")\n",
        "for bigram, prob in sorted_bigrams_by_prob[:10]:\n",
        "    print(f\"{bigram} -> {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gs73hhVmSPP",
        "outputId": "dbc92111-8bac-487f-a284-1ad0d07990cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Bigrams by Probability:\n",
            "28x28 grayscale -> 1.0000\n",
            "grayscale images -> 1.0000\n",
            "freely available -> 1.0000\n",
            "mutate that -> 1.0000\n",
            "asics known -> 1.0000\n",
            "tpus this -> 1.0000\n",
            "ranging from -> 1.0000\n",
            "tablets up -> 1.0000\n",
            "november 2015 -> 1.0000\n",
            "precipitation nowcasting -> 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 (3)"
      ],
      "metadata": {
        "id": "v56oEsqnmo0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "###############################################################################\n",
        "# 1. HELPER FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Lowercases the text, replaces punctuation with spaces,\n",
        "    and returns the cleaned string.\n",
        "    \"\"\"\n",
        "    processed_chars = []\n",
        "    for char in text.lower():\n",
        "        # Keep letters, digits, and spaces. Replace other chars with space.\n",
        "        if char in string.ascii_lowercase or char.isdigit() or char.isspace():\n",
        "            processed_chars.append(char)\n",
        "        else:\n",
        "            processed_chars.append(' ')\n",
        "    return \"\".join(processed_chars)\n",
        "\n",
        "def is_nounish(word):\n",
        "    \"\"\"\n",
        "    Extremely naive heuristic for 'noun-like' words:\n",
        "      - ends with 'tion', 'ment', 'ness', 'ty', 'ing'\n",
        "    \"\"\"\n",
        "    w = word.lower()\n",
        "    return (\n",
        "        w.endswith(\"tion\")\n",
        "        or w.endswith(\"ment\")\n",
        "        or w.endswith(\"ness\")\n",
        "        or w.endswith(\"ty\")\n",
        "        or w.endswith(\"ing\")\n",
        "    )\n",
        "\n",
        "def is_adjectivish(word):\n",
        "    \"\"\"\n",
        "    Extremely naive heuristic for 'adjective-like' words:\n",
        "      - ends with 'ive', 'ous', 'ic', 'al'\n",
        "    \"\"\"\n",
        "    w = word.lower()\n",
        "    return (\n",
        "        w.endswith(\"ive\")\n",
        "        or w.endswith(\"ous\")\n",
        "        or w.endswith(\"ic\")\n",
        "        or w.endswith(\"al\")\n",
        "    )\n",
        "\n",
        "def extract_naive_noun_phrases(text):\n",
        "    \"\"\"\n",
        "    Returns a list of 'noun phrases' based on very naive chunking:\n",
        "      - If a word meets 'is_nounish()' or 'is_adjectivish()',\n",
        "        group consecutive matches into a single chunk.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    noun_phrases = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for token in words:\n",
        "        if is_nounish(token) or is_adjectivish(token):\n",
        "            current_chunk.append(token)\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                noun_phrases.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "    # End chunk if it's still open\n",
        "    if current_chunk:\n",
        "        noun_phrases.append(\" \".join(current_chunk))\n",
        "    return noun_phrases\n",
        "\n",
        "###############################################################################\n",
        "# 2. READ THE CSV & EXTRACT REVIEWS\n",
        "###############################################################################\n",
        "\n",
        "filename = \"semantic_scholar_papers.csv\"  # Adjust path/name if needed\n",
        "MAX_REVIEWS = 100  # We'll limit to 100 reviews for demonstration\n",
        "\n",
        "reviews = []\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # skip header row\n",
        "\n",
        "    # Locate the column you want to treat as a \"review\"\n",
        "    if 'abstract' in header:\n",
        "        text_col_idx = header.index('abstract')\n",
        "    else:\n",
        "        # Fallback or pick another column if 'abstract' doesn't exist\n",
        "        text_col_idx = 0  # default to first column, for example\n",
        "\n",
        "    count_reviews = 0\n",
        "    for row in reader:\n",
        "        if text_col_idx < len(row):\n",
        "            text = row[text_col_idx]\n",
        "            if text:\n",
        "                reviews.append(text)\n",
        "                count_reviews += 1\n",
        "        if count_reviews >= MAX_REVIEWS:\n",
        "            break\n",
        "\n",
        "###############################################################################\n",
        "# 3. NAIVE NOUN PHRASE EXTRACTION & FREQUENCIES\n",
        "###############################################################################\n",
        "\n",
        "list_of_nps_per_review = []  # list of lists; each sub-list is the noun phrases for that review\n",
        "all_noun_phrases = set()\n",
        "\n",
        "for review_text in reviews:\n",
        "    cleaned = preprocess_text(review_text)\n",
        "    nps = extract_naive_noun_phrases(cleaned)\n",
        "    list_of_nps_per_review.append(nps)\n",
        "    all_noun_phrases.update(nps)\n",
        "\n",
        "all_noun_phrases = sorted(all_noun_phrases)  # consistent column ordering\n",
        "\n",
        "# Count occurrences: freq[i][np] = how many times np appears in review i\n",
        "freq = []\n",
        "for nps in list_of_nps_per_review:\n",
        "    count_map = {}\n",
        "    for np in nps:\n",
        "        count_map[np] = count_map.get(np, 0) + 1\n",
        "    freq.append(count_map)\n",
        "\n",
        "# Find max frequency of each noun phrase across all reviews\n",
        "max_freq = {}\n",
        "for np in all_noun_phrases:\n",
        "    max_val = 0\n",
        "    for i in range(len(reviews)):\n",
        "        max_val = max(max_val, freq[i].get(np, 0))\n",
        "    max_freq[np] = max_val\n",
        "\n",
        "# Compute relative probabilities\n",
        "#   P_rel = freq(np in review) / max_freq(np across entire dataset)\n",
        "relative_prob = []\n",
        "for i in range(len(reviews)):\n",
        "    row_probs = {}\n",
        "    for np in all_noun_phrases:\n",
        "        row_probs[np] = 0.0\n",
        "        if max_freq[np] > 0:\n",
        "            row_probs[np] = freq[i].get(np, 0) / max_freq[np]\n",
        "    relative_prob.append(row_probs)\n",
        "\n",
        "###############################################################################\n",
        "# 4A. CREATE & SHOW A PANDAS DATAFRAME\n",
        "###############################################################################\n",
        "\n",
        "df = pd.DataFrame(relative_prob)  # from list of dicts\n",
        "# Reorder columns\n",
        "df = df[all_noun_phrases]\n",
        "df.index = [f\"Review_{i+1}\" for i in range(len(reviews))]\n",
        "\n",
        "# Round to 2 decimals to keep output neat\n",
        "df = df.round(2)\n",
        "\n",
        "# Show the first 5 reviews Ã— first 10 noun phrases\n",
        "print(\"=== Partial View of Relative Noun Phrase Probabilities ===\")\n",
        "display(df.iloc[:5, :10])\n",
        "\n",
        "# Save full matrix to CSV\n",
        "df.to_csv(\"relative_probabilities.csv\", index=True)\n",
        "print(\"\\nFull DataFrame saved to 'relative_probabilities.csv' (Check left file pane in Colab).\")\n",
        "\n",
        "###############################################################################\n",
        "# 4B. PRINT TOP-N NOUN PHRASES PER REVIEW\n",
        "###############################################################################\n",
        "\n",
        "TOP_N = 5\n",
        "print(f\"\\n=== Top {TOP_N} Noun Phrases per Review ===\")\n",
        "for i, row_probs in enumerate(relative_prob):\n",
        "    review_name = f\"Review_{i+1}\"\n",
        "    # row_probs is {np: probability}, sort by descending probability\n",
        "    sorted_phrases = sorted(row_probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Keep top N with nonzero probabilities\n",
        "    top_phrases = [(p, val) for (p, val) in sorted_phrases if val > 0][:TOP_N]\n",
        "\n",
        "    print(f\"{review_name}:\")\n",
        "    if not top_phrases:\n",
        "        print(\"  (No noun phrases found)\\n\")\n",
        "        continue\n",
        "\n",
        "    for phrase, val in top_phrases:\n",
        "        print(f\"  {phrase} -> {val:.2f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MOuDwl1pmscu",
        "outputId": "784e1cfa-5106-4995-aea6-50e88a9b2d38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Partial View of Relative Noun Phrase Probabilities ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          ability  academic  academic teaching  accelerating  \\\n",
              "Review_1      0.0       0.0                0.0           0.0   \n",
              "Review_2      0.0       0.0                0.0           0.0   \n",
              "Review_3      0.0       0.0                0.0           0.0   \n",
              "Review_4      0.0       0.0                0.0           0.0   \n",
              "Review_5      0.0       0.0                0.0           0.0   \n",
              "\n",
              "          accelerating computational  accessibility  according  accounting  \\\n",
              "Review_1                         0.0            0.0        0.0         0.0   \n",
              "Review_2                         0.0            0.0        0.0         0.0   \n",
              "Review_3                         0.0            0.0        0.0         0.0   \n",
              "Review_4                         0.0            0.0        0.0         0.0   \n",
              "Review_5                         0.0            0.0        0.0         0.0   \n",
              "\n",
              "          achieving  acquiring  \n",
              "Review_1        0.0        0.0  \n",
              "Review_2        0.0        0.0  \n",
              "Review_3        0.0        0.0  \n",
              "Review_4        0.0        0.0  \n",
              "Review_5        0.0        0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98a93797-3efc-4728-b26a-7651f466e7b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>academic</th>\n",
              "      <th>academic teaching</th>\n",
              "      <th>accelerating</th>\n",
              "      <th>accelerating computational</th>\n",
              "      <th>accessibility</th>\n",
              "      <th>according</th>\n",
              "      <th>accounting</th>\n",
              "      <th>achieving</th>\n",
              "      <th>acquiring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Review_1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Review_2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Review_3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Review_4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Review_5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98a93797-3efc-4728-b26a-7651f466e7b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98a93797-3efc-4728-b26a-7651f466e7b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98a93797-3efc-4728-b26a-7651f466e7b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d29c014a-0095-4d44-b2d6-d8bd07fde567\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d29c014a-0095-4d44-b2d6-d8bd07fde567')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d29c014a-0095-4d44-b2d6-d8bd07fde567 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print()\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"ability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"academic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"academic teaching\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accelerating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accelerating computational\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accessibility\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"according\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accounting\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"achieving\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acquiring\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Full DataFrame saved to 'relative_probabilities.csv' (Check left file pane in Colab).\n",
            "\n",
            "=== Top 5 Noun Phrases per Review ===\n",
            "Review_1:\n",
            "  benchmarking -> 1.00\n",
            "  comprising -> 1.00\n",
            "  original -> 1.00\n",
            "  replacement -> 1.00\n",
            "  testing -> 1.00\n",
            "\n",
            "Review_2:\n",
            "  application -> 1.00\n",
            "  compelling -> 1.00\n",
            "  experiment -> 1.00\n",
            "  flexibility -> 1.00\n",
            "  heterogeneous -> 1.00\n",
            "\n",
            "Review_3:\n",
            "  conducting -> 1.00\n",
            "  deploying -> 1.00\n",
            "  executing -> 1.00\n",
            "  expressing -> 1.00\n",
            "  heterogeneous -> 1.00\n",
            "\n",
            "Review_4:\n",
            "  challenging -> 1.00\n",
            "  convolutional -> 1.00\n",
            "  crucial -> 1.00\n",
            "  extending -> 1.00\n",
            "  forecasting -> 1.00\n",
            "\n",
            "Review_5:\n",
            "  addition -> 1.00\n",
            "  application specific -> 1.00\n",
            "  biological -> 1.00\n",
            "  building -> 1.00\n",
            "  challenging -> 1.00\n",
            "\n",
            "Review_6:\n",
            "  accounting -> 1.00\n",
            "  addition -> 1.00\n",
            "  attempting -> 1.00\n",
            "  becoming -> 1.00\n",
            "  changing -> 1.00\n",
            "\n",
            "Review_7:\n",
            "  basic -> 1.00\n",
            "  commercial -> 1.00\n",
            "  hospital -> 1.00\n",
            "  including -> 1.00\n",
            "  individual -> 1.00\n",
            "\n",
            "Review_8:\n",
            "  academic -> 1.00\n",
            "  bringing -> 1.00\n",
            "  commercial -> 1.00\n",
            "  documentation -> 1.00\n",
            "  encouraging -> 1.00\n",
            "\n",
            "Review_9:\n",
            "  availability -> 1.00\n",
            "  growing technical -> 1.00\n",
            "  including -> 1.00\n",
            "  intensive -> 1.00\n",
            "  intersection -> 1.00\n",
            "\n",
            "Review_10:\n",
            "  discrimination -> 1.00\n",
            "  interpretability -> 1.00\n",
            "  position -> 1.00\n",
            "  rigorous -> 1.00\n",
            "  rigorous evaluation -> 1.00\n",
            "\n",
            "Review_11:\n",
            "  belonging -> 1.00\n",
            "  environment -> 1.00\n",
            "  hitting -> 1.00\n",
            "  involving -> 1.00\n",
            "  missing -> 1.00\n",
            "\n",
            "Review_12:\n",
            "  adaptive computation -> 1.00\n",
            "  basic -> 1.00\n",
            "  bibliographical -> 1.00\n",
            "  completion -> 1.00\n",
            "  dimensionality -> 1.00\n",
            "\n",
            "Review_13:\n",
            "  genetic -> 1.00\n",
            "  informal -> 1.00\n",
            "  mathematical -> 1.00\n",
            "  pascal -> 1.00\n",
            "  programming -> 1.00\n",
            "\n",
            "Review_14:\n",
            "  classification neural -> 1.00\n",
            "  combining -> 1.00\n",
            "  continuous -> 1.00\n",
            "  graphical -> 1.00\n",
            "  sequential -> 1.00\n",
            "\n",
            "Review_15:\n",
            "  activation -> 1.00\n",
            "  activation perturbation -> 1.00\n",
            "  al -> 1.00\n",
            "  attention -> 1.00\n",
            "  direction -> 1.00\n",
            "\n",
            "Review_16:\n",
            "  adversarial -> 1.00\n",
            "  appearing -> 1.00\n",
            "  capability -> 1.00\n",
            "  conducting -> 1.00\n",
            "  controlling -> 1.00\n",
            "\n",
            "Review_17:\n",
            "  allocation -> 1.00\n",
            "  appeal -> 1.00\n",
            "  automatic -> 1.00\n",
            "  convolutional neural -> 1.00\n",
            "  crucial -> 1.00\n",
            "\n",
            "Review_18:\n",
            "  (No noun phrases found)\n",
            "\n",
            "Review_19:\n",
            "  challenging -> 1.00\n",
            "  classifying -> 1.00\n",
            "  determining -> 1.00\n",
            "  examining -> 1.00\n",
            "  naive -> 1.00\n",
            "\n",
            "Review_20:\n",
            "  collection -> 1.00\n",
            "  consisting -> 1.00\n",
            "  national -> 1.00\n",
            "  recognition -> 1.00\n",
            "  optical -> 0.17\n",
            "\n",
            "Review_21:\n",
            "  addition -> 1.00\n",
            "  constructing -> 1.00\n",
            "  definitive -> 1.00\n",
            "  description -> 1.00\n",
            "  including -> 1.00\n",
            "\n",
            "Review_22:\n",
            "  acquiring excessive -> 1.00\n",
            "  building -> 1.00\n",
            "  dive -> 1.00\n",
            "  including -> 1.00\n",
            "  including convolutional -> 1.00\n",
            "\n",
            "Review_23:\n",
            "  availability -> 1.00\n",
            "  booming -> 1.00\n",
            "  categorization -> 1.00\n",
            "  consisting -> 1.00\n",
            "  digital -> 1.00\n",
            "\n",
            "Review_24:\n",
            "  adversarial training -> 1.00\n",
            "  construction -> 1.00\n",
            "  finding -> 1.00\n",
            "  leaking -> 1.00\n",
            "  malicious -> 1.00\n",
            "\n",
            "Review_25:\n",
            "  adaptive computation -> 1.00\n",
            "  additional material -> 1.00\n",
            "  allowing -> 1.00\n",
            "  basic -> 1.00\n",
            "  causal reasoning -> 1.00\n",
            "\n",
            "Review_26:\n",
            "  challenging -> 1.00\n",
            "  derivative -> 1.00\n",
            "  designing -> 1.00\n",
            "  distinctive setting -> 1.00\n",
            "  generation -> 1.00\n",
            "\n",
            "Review_27:\n",
            "  basic -> 1.00\n",
            "  building -> 1.00\n",
            "  having -> 1.00\n",
            "  improving -> 1.00\n",
            "  intuition -> 1.00\n",
            "\n",
            "Review_28:\n",
            "  additional -> 1.00\n",
            "  conditional -> 1.00\n",
            "  conditional probability -> 1.00\n",
            "  linguistic -> 1.00\n",
            "  representation -> 1.00\n",
            "\n",
            "Review_29:\n",
            "  ambiguity -> 1.00\n",
            "  causing uncertainty regarding -> 1.00\n",
            "  complexity turning -> 1.00\n",
            "  critical -> 1.00\n",
            "  demonstrating -> 1.00\n",
            "\n",
            "Review_30:\n",
            "  addition -> 1.00\n",
            "  classic semiparametric -> 1.00\n",
            "  classical setting -> 1.00\n",
            "  complexity -> 1.00\n",
            "  critical -> 1.00\n",
            "\n",
            "Review_31:\n",
            "  addition -> 1.00\n",
            "  additional -> 1.00\n",
            "  assessment -> 1.00\n",
            "  automation -> 1.00\n",
            "  boosting -> 1.00\n",
            "\n",
            "Review_32:\n",
            "  allowing crucial -> 1.00\n",
            "  collaborative learning -> 1.00\n",
            "  heterogeneous -> 1.00\n",
            "  local -> 1.00\n",
            "  pharmaceutical -> 1.00\n",
            "\n",
            "Review_33:\n",
            "  active -> 1.00\n",
            "  communication -> 1.00\n",
            "  complexity -> 1.00\n",
            "  curious -> 1.00\n",
            "  individual contribution -> 1.00\n",
            "\n",
            "Review_34:\n",
            "  focusing -> 1.00\n",
            "  increasing -> 1.00\n",
            "  modality -> 1.00\n",
            "  multimodal -> 1.00\n",
            "  representation translation alignment -> 1.00\n",
            "\n",
            "Review_35:\n",
            "  artificial -> 1.00\n",
            "  central -> 1.00\n",
            "  correlation -> 1.00\n",
            "  extending -> 1.00\n",
            "  heuristic -> 1.00\n",
            "\n",
            "Review_36:\n",
            "  adoption -> 1.00\n",
            "  automatic differentiation -> 1.00\n",
            "  capability -> 1.00\n",
            "  keeping -> 1.00\n",
            "  learning community -> 1.00\n",
            "\n",
            "Review_37:\n",
            "  computational -> 1.00\n",
            "  daunting -> 1.00\n",
            "  demanding -> 1.00\n",
            "  during -> 1.00\n",
            "  engineering -> 1.00\n",
            "\n",
            "Review_38:\n",
            "  distribution -> 1.00\n",
            "  goal -> 1.00\n",
            "  guiding -> 1.00\n",
            "  induction classification -> 1.00\n",
            "  interesting -> 1.00\n",
            "\n",
            "Review_39:\n",
            "  aggregation -> 1.00\n",
            "  assuming -> 1.00\n",
            "  basic -> 1.00\n",
            "  being -> 1.00\n",
            "  capturing -> 1.00\n",
            "\n",
            "Review_40:\n",
            "  addition -> 1.00\n",
            "  building -> 1.00\n",
            "  comprehensive -> 1.00\n",
            "  compromising -> 1.00\n",
            "  effective solution -> 1.00\n",
            "\n",
            "Review_41:\n",
            "  algorithmic -> 1.00\n",
            "  algorithmic fairness -> 1.00\n",
            "  being -> 1.00\n",
            "  crucial -> 1.00\n",
            "  discussing -> 1.00\n",
            "\n",
            "Review_42:\n",
            "  addition -> 1.00\n",
            "  classification representation -> 1.00\n",
            "  employment -> 1.00\n",
            "  extending -> 1.00\n",
            "  recognition -> 1.00\n",
            "\n",
            "Review_43:\n",
            "  arithmetic -> 1.00\n",
            "  collection -> 1.00\n",
            "  colluding -> 1.00\n",
            "  decimal -> 1.00\n",
            "  implement -> 1.00\n",
            "\n",
            "Review_44:\n",
            "  additive -> 1.00\n",
            "  causal -> 1.00\n",
            "  characterization -> 1.00\n",
            "  crucial -> 1.00\n",
            "  crucial topic -> 1.00\n",
            "\n",
            "Review_45:\n",
            "  classification using -> 1.00\n",
            "  clinical -> 1.00\n",
            "  convolutional neural -> 1.00\n",
            "  effective treatment -> 1.00\n",
            "  effectiveness -> 1.00\n",
            "\n",
            "Review_46:\n",
            "  learning consisting -> 1.00\n",
            "  public -> 1.00\n",
            "  learning -> 0.12\n",
            "\n",
            "Review_47:\n",
            "  algorithmic -> 1.00\n",
            "  beginning -> 1.00\n",
            "  computational complexity -> 1.00\n",
            "  convexity -> 1.00\n",
            "  emerging theoretical -> 1.00\n",
            "\n",
            "Review_48:\n",
            "  application specific -> 1.00\n",
            "  classifying -> 1.00\n",
            "  effective -> 1.00\n",
            "  emerging -> 1.00\n",
            "  give -> 1.00\n",
            "\n",
            "Review_49:\n",
            "  capacity -> 1.00\n",
            "  changing -> 1.00\n",
            "  changing society -> 1.00\n",
            "  classical -> 1.00\n",
            "  classical understanding -> 1.00\n",
            "\n",
            "Review_50:\n",
            "  accelerating -> 1.00\n",
            "  biological -> 1.00\n",
            "  discovering biological -> 1.00\n",
            "  expensive requiring -> 1.00\n",
            "  governing -> 1.00\n",
            "\n",
            "Review_51:\n",
            "  ethnicity -> 1.00\n",
            "  fairness -> 1.00\n",
            "  five -> 1.00\n",
            "  processing -> 1.00\n",
            "  social -> 1.00\n",
            "\n",
            "Review_52:\n",
            "  appealing -> 1.00\n",
            "  burgeoning -> 1.00\n",
            "  counterfactual -> 1.00\n",
            "  counterfactual explainability -> 1.00\n",
            "  counterfactual explanation -> 1.00\n",
            "\n",
            "Review_53:\n",
            "  advancing -> 1.00\n",
            "  augment -> 1.00\n",
            "  emerging -> 1.00\n",
            "  fundamental -> 1.00\n",
            "  industrial -> 1.00\n",
            "\n",
            "Review_54:\n",
            "  ability -> 1.00\n",
            "  addition -> 1.00\n",
            "  attention -> 1.00\n",
            "  constructing -> 1.00\n",
            "  defining -> 1.00\n",
            "\n",
            "Review_55:\n",
            "  computation -> 1.00\n",
            "  declarative symbolic -> 1.00\n",
            "  derive -> 1.00\n",
            "  differentiation -> 1.00\n",
            "  imperative -> 1.00\n",
            "\n",
            "Review_56:\n",
            "  anatomical -> 1.00\n",
            "  diagnostic -> 1.00\n",
            "  looking -> 1.00\n",
            "  learning -> 0.12\n",
            "\n",
            "Review_57:\n",
            "  ability -> 1.00\n",
            "  algorithmic society meaning -> 1.00\n",
            "  becoming -> 1.00\n",
            "  expanding accelerating -> 1.00\n",
            "  explanation -> 1.00\n",
            "\n",
            "Review_58:\n",
            "  addition -> 1.00\n",
            "  aerial -> 1.00\n",
            "  attention -> 1.00\n",
            "  being -> 1.00\n",
            "  belonging -> 1.00\n",
            "\n",
            "Review_59:\n",
            "  accelerating -> 1.00\n",
            "  basic notion -> 1.00\n",
            "  computing -> 1.00\n",
            "  conceptual -> 1.00\n",
            "  fertilization -> 1.00\n",
            "\n",
            "Review_60:\n",
            "  accelerating computational -> 1.00\n",
            "  achieving -> 1.00\n",
            "  amplifying -> 1.00\n",
            "  coaction -> 1.00\n",
            "  computational -> 1.00\n",
            "\n",
            "Review_61:\n",
            "  facial recognition -> 1.00\n",
            "  formal definition -> 1.00\n",
            "  recommendation -> 1.00\n",
            "  sentiment -> 1.00\n",
            "  medical -> 0.50\n",
            "\n",
            "Review_62:\n",
            "  action -> 1.00\n",
            "  agricultural production -> 1.00\n",
            "  animal -> 1.00\n",
            "  detection -> 1.00\n",
            "  evolving -> 1.00\n",
            "\n",
            "Review_63:\n",
            "  consumption -> 1.00\n",
            "  essential -> 1.00\n",
            "  generating -> 1.00\n",
            "  making accounting -> 1.00\n",
            "  mitigation -> 1.00\n",
            "\n",
            "Review_64:\n",
            "  ability -> 1.00\n",
            "  al -> 1.00\n",
            "  analogous -> 1.00\n",
            "  artificial neural -> 1.00\n",
            "  demonstrating -> 1.00\n",
            "\n",
            "Review_65:\n",
            "  canonical -> 1.00\n",
            "  comparative -> 1.00\n",
            "  doubling -> 1.00\n",
            "  economic -> 1.00\n",
            "  empirical -> 1.00\n",
            "\n",
            "Review_66:\n",
            "  application -> 1.00\n",
            "  collecting analysing interpreting -> 1.00\n",
            "  deduction -> 1.00\n",
            "  descriptive -> 1.00\n",
            "  drawing -> 1.00\n",
            "\n",
            "Review_67:\n",
            "  competition -> 1.00\n",
            "  evaluating derivative -> 1.00\n",
            "  final -> 1.00\n",
            "  intervention -> 1.00\n",
            "  objective -> 1.00\n",
            "\n",
            "Review_68:\n",
            "  classification -> 1.00\n",
            "  computational complexity -> 1.00\n",
            "  continuous function -> 1.00\n",
            "  conventional -> 1.00\n",
            "  following -> 1.00\n",
            "\n",
            "Review_69:\n",
            "  aiming -> 1.00\n",
            "  combination -> 1.00\n",
            "  contribution -> 1.00\n",
            "  integration -> 1.00\n",
            "  providing -> 1.00\n",
            "\n",
            "Review_70:\n",
            "  ambiguity -> 1.00\n",
            "  arising -> 1.00\n",
            "  deployment -> 1.00\n",
            "  electronic -> 1.00\n",
            "  instability -> 1.00\n",
            "\n",
            "Review_71:\n",
            "  analytic -> 1.00\n",
            "  embedding -> 1.00\n",
            "  freeing -> 1.00\n",
            "  measurement -> 1.00\n",
            "  measurement minimizing -> 1.00\n",
            "\n",
            "Review_72:\n",
            "  actual -> 1.00\n",
            "  comprehensive statistical -> 1.00\n",
            "  polynomial -> 1.00\n",
            "  various -> 0.50\n",
            "  using -> 0.33\n",
            "\n",
            "Review_73:\n",
            "  according -> 1.00\n",
            "  essential -> 1.00\n",
            "  making -> 1.00\n",
            "  perceive -> 1.00\n",
            "  sensitive -> 1.00\n",
            "\n",
            "Review_74:\n",
            "  chemical -> 1.00\n",
            "  computational complexity -> 1.00\n",
            "  constructing -> 1.00\n",
            "  generation -> 1.00\n",
            "  notion -> 1.00\n",
            "\n",
            "Review_75:\n",
            "  artificial -> 1.00\n",
            "  being -> 1.00\n",
            "  extensive technical -> 1.00\n",
            "  focusing -> 1.00\n",
            "  goal -> 1.00\n",
            "\n",
            "Review_76:\n",
            "  action -> 1.00\n",
            "  astounding -> 1.00\n",
            "  critical -> 1.00\n",
            "  driving -> 1.00\n",
            "  educating -> 1.00\n",
            "\n",
            "Review_77:\n",
            "  academic -> 1.00\n",
            "  agnostic -> 1.00\n",
            "  asking -> 1.00\n",
            "  business -> 1.00\n",
            "  complexity -> 1.00\n",
            "\n",
            "Review_78:\n",
            "  application programming -> 1.00\n",
            "  clustering classification -> 1.00\n",
            "  allowing -> 0.50\n",
            "  general -> 0.50\n",
            "  extraction -> 0.33\n",
            "\n",
            "Review_79:\n",
            "  adversarial perturbation -> 1.00\n",
            "  autonomous -> 1.00\n",
            "  competitive -> 1.00\n",
            "  final -> 1.00\n",
            "  perturbation -> 1.00\n",
            "\n",
            "Review_80:\n",
            "  allying -> 1.00\n",
            "  belonging -> 1.00\n",
            "  cleaning -> 1.00\n",
            "  comparative -> 1.00\n",
            "  competitive -> 1.00\n",
            "\n",
            "Review_81:\n",
            "  algorithmic fairness -> 1.00\n",
            "  explainability -> 1.00\n",
            "  foundational -> 1.00\n",
            "  internal -> 1.00\n",
            "  providing -> 1.00\n",
            "\n",
            "Review_82:\n",
            "  community -> 1.00\n",
            "  experimental -> 1.00\n",
            "  initiative -> 1.00\n",
            "  international -> 1.00\n",
            "  learning reproducibility -> 1.00\n",
            "\n",
            "Review_83:\n",
            "  academic -> 1.00\n",
            "  addressing -> 1.00\n",
            "  business -> 1.00\n",
            "  corresponding -> 1.00\n",
            "  deploying -> 1.00\n",
            "\n",
            "Review_84:\n",
            "  advancement -> 1.00\n",
            "  ethical -> 1.00\n",
            "  numerous ethical -> 1.00\n",
            "  ongoing -> 1.00\n",
            "  postdeployment -> 1.00\n",
            "\n",
            "Review_85:\n",
            "  algorithmic differentiation -> 1.00\n",
            "  atmospheric -> 1.00\n",
            "  automatic differentiation -> 1.00\n",
            "  backpropagation -> 1.00\n",
            "  bring clarity -> 1.00\n",
            "\n",
            "Review_86:\n",
            "  acquiring -> 1.00\n",
            "  addition -> 1.00\n",
            "  annotating -> 1.00\n",
            "  availability -> 1.00\n",
            "  clustering -> 1.00\n",
            "\n",
            "Review_87:\n",
            "  beneficial -> 1.00\n",
            "  colorimetric -> 1.00\n",
            "  convolutional neural -> 1.00\n",
            "  critical -> 1.00\n",
            "  detection -> 1.00\n",
            "\n",
            "Review_88:\n",
            "  application -> 1.00\n",
            "  foundation -> 1.00\n",
            "  systematic organization -> 1.00\n",
            "  comprehensive -> 0.50\n",
            "  learning -> 0.50\n",
            "\n",
            "Review_89:\n",
            "  defining -> 1.00\n",
            "  distribution -> 1.00\n",
            "  encouraging experimental -> 1.00\n",
            "  fraction -> 1.00\n",
            "  goal -> 1.00\n",
            "\n",
            "Review_90:\n",
            "  capacity -> 1.00\n",
            "  conflicting -> 1.00\n",
            "  dimensionality -> 1.00\n",
            "  effective -> 1.00\n",
            "  implementing -> 1.00\n",
            "\n",
            "Review_91:\n",
            "  ensembling -> 1.00\n",
            "  incorporating external -> 1.00\n",
            "  offering -> 1.00\n",
            "  probabilistic forecasting -> 1.00\n",
            "  providing -> 1.00\n",
            "\n",
            "Review_92:\n",
            "  addition -> 1.00\n",
            "  being -> 1.00\n",
            "  biomedical -> 1.00\n",
            "  driving technological -> 1.00\n",
            "  economic -> 1.00\n",
            "\n",
            "Review_93:\n",
            "  accessibility -> 1.00\n",
            "  clinical -> 1.00\n",
            "  regarding reproducibility -> 1.00\n",
            "  several reproducibility -> 1.00\n",
            "  learning -> 0.62\n",
            "\n",
            "Review_94:\n",
            "  academic teaching -> 1.00\n",
            "  combustion -> 1.00\n",
            "  contribution -> 1.00\n",
            "  exhaustive characterization -> 1.00\n",
            "  involving -> 1.00\n",
            "\n",
            "Review_95:\n",
            "  adding -> 1.00\n",
            "  augmentation -> 1.00\n",
            "  benchmarking -> 1.00\n",
            "  building -> 1.00\n",
            "  detecting -> 1.00\n",
            "\n",
            "Review_96:\n",
            "  ability -> 1.00\n",
            "  anecdotal -> 1.00\n",
            "  connection -> 1.00\n",
            "  effective -> 1.00\n",
            "  empirical -> 1.00\n",
            "\n",
            "Review_97:\n",
            "  assisting -> 1.00\n",
            "  being -> 1.00\n",
            "  clinical -> 1.00\n",
            "  critical -> 1.00\n",
            "  effective -> 1.00\n",
            "\n",
            "Review_98:\n",
            "  analytic -> 1.00\n",
            "  applying -> 1.00\n",
            "  astonishing -> 1.00\n",
            "  basic -> 1.00\n",
            "  being -> 1.00\n",
            "\n",
            "Review_99:\n",
            "  clustering -> 1.00\n",
            "  debugging -> 1.00\n",
            "  developing -> 1.00\n",
            "  environment -> 1.00\n",
            "  performing -> 1.00\n",
            "\n",
            "Review_100:\n",
            "  being -> 1.00\n",
            "  classical learning -> 1.00\n",
            "  commercial -> 1.00\n",
            "  confidential -> 1.00\n",
            "  confidentiality -> 1.00\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any **pre-existing libraries** to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LjN0iysvo9-n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "ac1f0a01-4825-421c-cdde-6097ceb8d188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Partial TFâ€“IDF Matrix (5 rows Ã— 10 columns) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0        000         1        10       100  1004  10Ã—  10â€“fold  \\\n",
              "Doc_1  0.000000  15.648092  0.000000  6.437752  0.000000   0.0  0.0      0.0   \n",
              "Doc_2  0.000000   0.000000  0.000000  0.000000  0.000000   0.0  0.0      0.0   \n",
              "Doc_3  3.506558   0.000000  0.000000  0.000000  0.000000   0.0  0.0      0.0   \n",
              "Doc_4  0.000000   0.000000  0.000000  0.000000  0.000000   0.0  0.0      0.0   \n",
              "Doc_5  0.000000   0.000000  2.040221  0.000000  3.912023   0.0  0.0      0.0   \n",
              "\n",
              "        11   13  \n",
              "Doc_1  0.0  0.0  \n",
              "Doc_2  0.0  0.0  \n",
              "Doc_3  0.0  0.0  \n",
              "Doc_4  0.0  0.0  \n",
              "Doc_5  0.0  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4aa020e-734d-417f-b921-08a21f502105\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>000</th>\n",
              "      <th>1</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1004</th>\n",
              "      <th>10Ã—</th>\n",
              "      <th>10â€“fold</th>\n",
              "      <th>11</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc_1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.648092</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.437752</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_3</th>\n",
              "      <td>3.506558</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.040221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.912023</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4aa020e-734d-417f-b921-08a21f502105')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4aa020e-734d-417f-b921-08a21f502105 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4aa020e-734d-417f-b921-08a21f502105');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9e3094a6-4a74-4eb7-a22f-4a30180e7658\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e3094a6-4a74-4eb7-a22f-4a30180e7658')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9e3094a6-4a74-4eb7-a22f-4a30180e7658 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Full TF\\u2013IDF matrix saved as 'tfidf_matrix\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5681803650892416,\n        \"min\": 0.0,\n        \"max\": 3.506557897319982,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.506557897319982,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"000\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.99803949574429,\n        \"min\": 0.0,\n        \"max\": 15.648092021712584,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          15.648092021712584\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9124144923392636,\n        \"min\": 0.0,\n        \"max\": 2.0402208285265546,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.0402208285265546,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8790500622144015,\n        \"min\": 0.0,\n        \"max\": 6.437751649736401,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          6.437751649736401\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"100\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7495098739360724,\n        \"min\": 0.0,\n        \"max\": 3.912023005428146,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.912023005428146,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1004\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\\u00d7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\\u2013fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full TFâ€“IDF matrix saved as 'tfidf_matrix.csv' in your Colab environment.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import math\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "###############################################################################\n",
        "# 1. HELPER FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Lowercases text, replaces punctuation with spaces, and strips extra spaces.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Replace punctuation with space\n",
        "    for p in string.punctuation:\n",
        "        text = text.replace(p, \" \")\n",
        "    # You could also remove digits or handle them differently if desired\n",
        "    # Remove extra spaces\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "###############################################################################\n",
        "# 2. READ THE CSV & EXTRACT DOCUMENTS\n",
        "###############################################################################\n",
        "\n",
        "filename = \"semantic_scholar_papers.csv\"  # Adjust to your file\n",
        "MAX_DOCS = 100  # If you want to limit to 100 documents for demonstration\n",
        "\n",
        "documents = []\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # skip header row\n",
        "\n",
        "    # Try to locate a column named \"abstract\"; if not found, fallback to first column\n",
        "    if \"abstract\" in header:\n",
        "        text_col_idx = header.index(\"abstract\")\n",
        "    else:\n",
        "        text_col_idx = 0  # fallback if no 'abstract' column\n",
        "\n",
        "    count_docs = 0\n",
        "    for row in reader:\n",
        "        if text_col_idx < len(row):\n",
        "            text = row[text_col_idx]\n",
        "            if text:\n",
        "                documents.append(text)\n",
        "                count_docs += 1\n",
        "        if count_docs >= MAX_DOCS:\n",
        "            break\n",
        "\n",
        "# Preprocess each document\n",
        "documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "###############################################################################\n",
        "# 3. TOKENIZE & BUILD TF (TERM FREQUENCY)\n",
        "###############################################################################\n",
        "# docs_terms[i] = a dictionary mapping term -> frequency in doc i\n",
        "# vocab = set of all unique terms across all docs\n",
        "\n",
        "docs_terms = []\n",
        "vocab_set = set()\n",
        "\n",
        "for doc in documents:\n",
        "    words = doc.split()\n",
        "    freq_dict = {}\n",
        "    for w in words:\n",
        "        freq_dict[w] = freq_dict.get(w, 0) + 1\n",
        "    docs_terms.append(freq_dict)\n",
        "    vocab_set.update(freq_dict.keys())\n",
        "\n",
        "vocab_list = sorted(vocab_set)  # sorted list of all unique terms in the dataset\n",
        "\n",
        "###############################################################################\n",
        "# 4. CALCULATE IDF (INVERSE DOCUMENT FREQUENCY)\n",
        "###############################################################################\n",
        "# IDF(t) = log( N / DF(t) ),\n",
        "# where N is total # of documents, DF(t) is # of documents that contain term t\n",
        "\n",
        "N = len(documents)\n",
        "df_dict = {}  # DF for each term: how many docs contain it\n",
        "for term in vocab_list:\n",
        "    df = sum(1 for freq_dict in docs_terms if term in freq_dict)\n",
        "    df_dict[term] = df\n",
        "\n",
        "idf_dict = {}\n",
        "for term, df in df_dict.items():\n",
        "    # We add a tiny check to avoid log(0)\n",
        "    if df > 0:\n",
        "        idf_dict[term] = math.log(N / df)\n",
        "    else:\n",
        "        idf_dict[term] = 0.0  # theoretically shouldn't happen if the term is in the vocab\n",
        "\n",
        "###############################################################################\n",
        "# 5. BUILD THE TFâ€“IDF MATRIX\n",
        "###############################################################################\n",
        "# We'll store the matrix as a list of lists, then convert to a Pandas DataFrame\n",
        "# tf_idf_matrix[i][j] = TF(document_i, term_j) * IDF(term_j)\n",
        "\n",
        "tf_idf_matrix = []\n",
        "for i, freq_dict in enumerate(docs_terms):\n",
        "    row = []\n",
        "    for term in vocab_list:\n",
        "        tf = freq_dict.get(term, 0)  # raw frequency in doc i\n",
        "        idf = idf_dict[term]\n",
        "        tf_idf_value = tf * idf\n",
        "        row.append(tf_idf_value)\n",
        "    tf_idf_matrix.append(row)\n",
        "\n",
        "###############################################################################\n",
        "# 6. (OPTIONAL) CREATE A PANDAS DATAFRAME & SHOW PART OF IT\n",
        "###############################################################################\n",
        "\n",
        "df_tfidf = pd.DataFrame(tf_idf_matrix, columns=vocab_list)\n",
        "df_tfidf.index = [f\"Doc_{i+1}\" for i in range(N)]\n",
        "\n",
        "# Let's just look at a small slice: first 5 documents Ã— first 10 terms\n",
        "print(\"=== Partial TFâ€“IDF Matrix (5 rows Ã— 10 columns) ===\")\n",
        "display(df_tfidf.iloc[:5, :10])\n",
        "\n",
        "# If you want to save the full TFâ€“IDF table to CSV to inspect elsewhere:\n",
        "df_tfidf.to_csv(\"tfidf_matrix.csv\")\n",
        "print(\"Full TFâ€“IDF matrix saved as 'tfidf_matrix.csv' in your Colab environment.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 (2)"
      ],
      "metadata": {
        "id": "qoZurLR-odnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "import string\n",
        "\n",
        "###############################################################################\n",
        "# 1. HELPER FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Lowercases 'text', replaces punctuation with spaces, and strips extra whitespace.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Replace punctuation with spaces\n",
        "    for p in string.punctuation:\n",
        "        text = text.replace(p, \" \")\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "def compute_tf(doc_tokens):\n",
        "    \"\"\"\n",
        "    Given a list of tokens from a single document,\n",
        "    return a dictionary: {term: term_frequency_in_doc}\n",
        "    \"\"\"\n",
        "    freq_dict = {}\n",
        "    for tok in doc_tokens:\n",
        "        freq_dict[tok] = freq_dict.get(tok, 0) + 1\n",
        "    return freq_dict\n",
        "\n",
        "def dot_product(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the dot product of two vectors represented as dicts of {term: weight}.\n",
        "    Only terms that appear in both vectors contribute.\n",
        "    \"\"\"\n",
        "    total = 0.0\n",
        "    # Iterate over the smaller dict for efficiency if desired\n",
        "    if len(vec1) < len(vec2):\n",
        "        for term, weight1 in vec1.items():\n",
        "            weight2 = vec2.get(term, 0.0)\n",
        "            total += weight1 * weight2\n",
        "    else:\n",
        "        for term, weight2 in vec2.items():\n",
        "            weight1 = vec1.get(term, 0.0)\n",
        "            total += weight1 * weight2\n",
        "    return total\n",
        "\n",
        "def magnitude(vec):\n",
        "    \"\"\"\n",
        "    Euclidean norm of a vector represented as {term: weight}.\n",
        "    sqrt( sum( weight^2 ) )\n",
        "    \"\"\"\n",
        "    return math.sqrt(sum(w * w for w in vec.values()))\n",
        "\n",
        "def cosine_similarity(vecA, vecB):\n",
        "    \"\"\"\n",
        "    Cosine similarity = dot(vecA, vecB) / (||vecA|| * ||vecB||)\n",
        "    \"\"\"\n",
        "    denom = magnitude(vecA) * magnitude(vecB)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return dot_product(vecA, vecB) / denom\n",
        "\n",
        "###############################################################################\n",
        "# 2. LOAD DOCUMENTS FROM CSV\n",
        "###############################################################################\n",
        "\n",
        "filename = \"semantic_scholar_papers.csv\"  # Update path/name if needed\n",
        "MAX_DOCS = 100  # We'll limit to 100 documents for demonstration\n",
        "documents = []  # Each item will be a string of text (e.g., an abstract)\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # Skip header row\n",
        "\n",
        "    # Identify the text column. For example, 'abstract' if it exists.\n",
        "    if 'abstract' in header:\n",
        "        text_col_idx = header.index('abstract')\n",
        "    else:\n",
        "        # Fallback if no 'abstract' column is found; pick another text column or 0\n",
        "        text_col_idx = 0\n",
        "\n",
        "    count_docs = 0\n",
        "    for row in reader:\n",
        "        if text_col_idx < len(row):\n",
        "            text = row[text_col_idx]\n",
        "            if text.strip():\n",
        "                documents.append(text.strip())\n",
        "                count_docs += 1\n",
        "        if count_docs >= MAX_DOCS:\n",
        "            break\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "###############################################################################\n",
        "# 3. BUILD THE TFâ€“IDF MATRIX (FROM SCRATCH)\n",
        "###############################################################################\n",
        "\n",
        "# STEP 3A: Preprocess each document and compute TF maps\n",
        "preprocessed_docs = []\n",
        "doc_term_freqs = []         # list of dicts, each dict = {term: count_in_doc}\n",
        "for doc_text in documents:\n",
        "    # Preprocess text (lowercase, remove punctuation, etc.)\n",
        "    cleaned = preprocess_text(doc_text)\n",
        "    tokens = cleaned.split()\n",
        "    tf_dict = compute_tf(tokens)\n",
        "    doc_term_freqs.append(tf_dict)\n",
        "    preprocessed_docs.append(tokens)\n",
        "\n",
        "# Collect all unique terms in the dataset\n",
        "all_terms = set()\n",
        "for tf_map in doc_term_freqs:\n",
        "    all_terms.update(tf_map.keys())\n",
        "all_terms = sorted(all_terms)\n",
        "\n",
        "# STEP 3B: Compute document frequency (DF) = in how many docs each term appears\n",
        "doc_count = len(documents)\n",
        "doc_freq = {term: 0 for term in all_terms}\n",
        "\n",
        "for term in all_terms:\n",
        "    for tf_map in doc_term_freqs:\n",
        "        if term in tf_map:\n",
        "            doc_freq[term] += 1\n",
        "\n",
        "# STEP 3C: Compute IDF for each term\n",
        "# We'll use the standard formula IDF = log( N / DF ), using log base e\n",
        "idf = {}\n",
        "for term in all_terms:\n",
        "    df_t = doc_freq[term]\n",
        "    if df_t == 0:\n",
        "        # If a term never appears, skip or set IDF = 0\n",
        "        idf[term] = 0.0\n",
        "    else:\n",
        "        idf[term] = math.log(doc_count / df_t)\n",
        "\n",
        "# STEP 3D: Compute TFâ€“IDF for each document\n",
        "# We'll store each document as a dict {term: tf-idf weight}\n",
        "tfidf_docs = []\n",
        "\n",
        "for i, tf_map in enumerate(doc_term_freqs):\n",
        "    # Convert raw counts -> TF (raw_count / total_tokens_in_doc),\n",
        "    # then multiply by IDF\n",
        "    doc_vec = {}\n",
        "    total_tokens = sum(tf_map.values())\n",
        "    for term, count in tf_map.items():\n",
        "        tf_val = count / total_tokens\n",
        "        doc_vec[term] = tf_val * idf[term]\n",
        "    tfidf_docs.append(doc_vec)\n",
        "\n",
        "###############################################################################\n",
        "# 4. RANK DOCUMENTS FOR A GIVEN QUERY USING COSINE SIMILARITY\n",
        "###############################################################################\n",
        "\n",
        "# EXAMPLE QUERY (adjust to your own):\n",
        "query_text = \"An outstanding movie with a haunting performance and best character development\"\n",
        "\n",
        "# STEP 4A: Preprocess and compute TF for the query\n",
        "cleaned_query = preprocess_text(query_text)\n",
        "query_tokens = cleaned_query.split()\n",
        "query_tf_map = compute_tf(query_tokens)\n",
        "\n",
        "# STEP 4B: Convert query TF -> TF-IDF (using the same IDF from the corpus)\n",
        "query_vec = {}\n",
        "total_query_tokens = sum(query_tf_map.values())\n",
        "for term, qcount in query_tf_map.items():\n",
        "    if term in idf:\n",
        "        tf_val = qcount / total_query_tokens  # query TF\n",
        "        query_vec[term] = tf_val * idf[term]  # multiply by IDF\n",
        "\n",
        "# STEP 4C: Compute cosine similarity with each document vector\n",
        "doc_scores = []\n",
        "for i, doc_vec in enumerate(tfidf_docs):\n",
        "    score = cosine_similarity(doc_vec, query_vec)\n",
        "    doc_scores.append((i, score))\n",
        "\n",
        "# STEP 4D: Sort documents by descending similarity\n",
        "doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# STEP 4E: Print the top 5 documents (or however many you want)\n",
        "print(\"\\nTop 5 Documents for Query:\", query_text)\n",
        "for rank, (doc_idx, score) in enumerate(doc_scores[:5], start=1):\n",
        "    snippet = documents[doc_idx][:150].replace(\"\\n\", \" \")\n",
        "    print(f\"Rank #{rank} | Doc Index = {doc_idx} | Cosine Sim = {score:.4f} | Snippet = {snippet}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiUclWr_ogsI",
        "outputId": "a196ce81-8ef3-4f01-ac20-f338d5ed3ca2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 documents.\n",
            "\n",
            "Top 5 Documents for Query: An outstanding movie with a haunting performance and best character development\n",
            "Rank #1 | Doc Index = 19 | Cosine Sim = 0.1599 | Snippet = In this issue, â€œBest of the Webâ€ presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of...\n",
            "Rank #2 | Doc Index = 18 | Cosine Sim = 0.1055 | Snippet = We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. U...\n",
            "Rank #3 | Doc Index = 28 | Cosine Sim = 0.0428 | Snippet = Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman...\n",
            "Rank #4 | Doc Index = 71 | Cosine Sim = 0.0422 | Snippet = Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find ...\n",
            "Rank #5 | Doc Index = 64 | Cosine Sim = 0.0374 | Snippet = We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit or Fine tune bert model).\n",
        "\n",
        "(2) Visualize the word embedding model you created. (PCA and T-sne)\n",
        "\n",
        "(3) Calculate the cosine similarity between a few pairs of words to see if the model captures semantic similarity accurately.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eczZgyAoo05Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177cc57a-204d-4ac4-ba17-645f1e0b4479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 documents.\n",
            "Total tokens (raw): 17167\n",
            "Vocab size (words with freq >= 3): 1096\n",
            "Total tokens (filtered): 14642\n",
            "Total skip-gram pairs: 58562\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/1 completed.\n",
            "Training done.\n",
            "\n",
            "Example embedding for a few words in your vocab:\n",
            "machine => [-0.021, 0.005, -0.035, -0.128, -0.035, -0.067, -0.085, -0.157, 0.009, -0.071, ... ]\n",
            "learning => [-0.025, 0.006, -0.041, -0.154, -0.041, -0.080, -0.103, -0.193, 0.008, -0.086, ... ]\n",
            "data => [-0.016, 0.007, -0.026, -0.104, -0.025, -0.053, -0.072, -0.142, -0.004, -0.058, ... ]\n",
            "model => [-0.009, 0.003, -0.013, -0.050, -0.012, -0.024, -0.035, -0.068, -0.000, -0.027, ... ]\n",
            "neural => [-0.006, 0.002, -0.008, -0.031, -0.007, -0.016, -0.021, -0.043, -0.001, -0.016, ... ]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "###############################################################################\n",
        "# 1. CONFIGURATION\n",
        "###############################################################################\n",
        "\n",
        "CSV_FILE = \"semantic_scholar_papers.csv\"  # Change if needed\n",
        "TEXT_COLUMN_NAME = \"abstract\"  # For your assignment 2 dataset\n",
        "MAX_DOCS = 100               # Only load this many docs from the CSV\n",
        "EMBED_DIM = 300              # Dimension of word embeddings\n",
        "WINDOW_SIZE = 2             # How many words on each side form the context\n",
        "MIN_FREQ = 3                # Minimum frequency to keep a word in vocab\n",
        "NEGATIVE_SAMPLES = 5        # Number of negative samples per positive pair\n",
        "EPOCHS = 1                  # Number of passes over the data (increase if you want)\n",
        "LEARNING_RATE = 0.025       # Initial learning rate\n",
        "DECAY = 1e-5                # LR decay factor per iteration (very simplistic)\n",
        "\n",
        "###############################################################################\n",
        "# 2. READ & PREPROCESS TEXT DATA\n",
        "###############################################################################\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase and replace punctuation with spaces\n",
        "    text = text.lower()\n",
        "    for p in string.punctuation:\n",
        "        text = text.replace(p, \" \")\n",
        "    # Collapse multiple spaces\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "def load_documents(csv_file, text_col, max_docs):\n",
        "    docs = []\n",
        "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)  # skip\n",
        "        if text_col in header:\n",
        "            col_idx = header.index(text_col)\n",
        "        else:\n",
        "            col_idx = 0  # fallback\n",
        "\n",
        "        count = 0\n",
        "        for row in reader:\n",
        "            if col_idx < len(row):\n",
        "                raw_text = row[col_idx].strip()\n",
        "                if raw_text:\n",
        "                    docs.append(raw_text)\n",
        "                    count += 1\n",
        "            if count >= max_docs:\n",
        "                break\n",
        "    return docs\n",
        "\n",
        "# Load the text\n",
        "documents = load_documents(CSV_FILE, TEXT_COLUMN_NAME, MAX_DOCS)\n",
        "print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "# Concatenate all text for training\n",
        "all_text = \" \".join(documents)\n",
        "all_text = preprocess_text(all_text)\n",
        "tokens = all_text.split()\n",
        "\n",
        "print(f\"Total tokens (raw): {len(tokens)}\")\n",
        "\n",
        "###############################################################################\n",
        "# 3. BUILD VOCABULARY\n",
        "###############################################################################\n",
        "\n",
        "# Count word frequencies\n",
        "freqs = Counter(tokens)\n",
        "\n",
        "# Filter out rare words\n",
        "vocab = [w for w, c in freqs.items() if c >= MIN_FREQ]\n",
        "# Sort vocab by frequency (highest first), but not strictly necessary\n",
        "vocab = sorted(vocab, key=lambda w: freqs[w], reverse=True)\n",
        "\n",
        "# Assign each word an index\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "print(f\"Vocab size (words with freq >= {MIN_FREQ}): {len(vocab)}\")\n",
        "\n",
        "# Rebuild the token list with unknown words filtered out\n",
        "filtered_tokens = [w for w in tokens if w in word2idx]\n",
        "print(f\"Total tokens (filtered): {len(filtered_tokens)}\")\n",
        "\n",
        "###############################################################################\n",
        "# 4. PREPARE NEGATIVE SAMPLING DISTRIBUTION\n",
        "###############################################################################\n",
        "# A standard approach is to sample words according to their frequency^0.75\n",
        "# (as recommended in the original Word2Vec paper)\n",
        "\n",
        "word_counts = [freqs[w] for w in vocab]  # frequencies for each vocab word\n",
        "power = 0.75\n",
        "weighted_counts = [c**power for c in word_counts]\n",
        "sum_wc = float(sum(weighted_counts))\n",
        "neg_sample_prob = [wc / sum_wc for wc in weighted_counts]\n",
        "\n",
        "# Create a cumulative distribution for sampling\n",
        "cdf = []\n",
        "cumulative = 0.0\n",
        "for p in neg_sample_prob:\n",
        "    cumulative += p\n",
        "    cdf.append(cumulative)\n",
        "\n",
        "def sample_negative():\n",
        "    \"\"\"Sample a single negative word index based on the alias method (cdf).\"\"\"\n",
        "    x = random.random()\n",
        "    # binary search to find x in cdf\n",
        "    # or do a linear search for simplicity\n",
        "    # (for large vocab, you'd want something more efficient).\n",
        "    low, high = 0, len(cdf) - 1\n",
        "    while low < high:\n",
        "        mid = (low + high) // 2\n",
        "        if x > cdf[mid]:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid\n",
        "    return low  # index of sampled word in vocab\n",
        "\n",
        "###############################################################################\n",
        "# 5. CREATE TRAINING SAMPLES (CENTER, CONTEXT) PAIRS\n",
        "###############################################################################\n",
        "# We'll do this once upfront (though for very large corpora you'd generate on the fly).\n",
        "\n",
        "training_pairs = []\n",
        "for i, w in enumerate(filtered_tokens):\n",
        "    center_idx = word2idx[w]\n",
        "    # define context window\n",
        "    start = max(0, i - WINDOW_SIZE)\n",
        "    end = min(len(filtered_tokens), i + WINDOW_SIZE + 1)\n",
        "    for j in range(start, end):\n",
        "        if j != i:  # skip center word itself\n",
        "            context_word = filtered_tokens[j]\n",
        "            context_idx = word2idx[context_word]\n",
        "            training_pairs.append((center_idx, context_idx))\n",
        "\n",
        "print(f\"Total skip-gram pairs: {len(training_pairs)}\")\n",
        "\n",
        "###############################################################################\n",
        "# 6. INITIALIZE EMBEDDINGS\n",
        "###############################################################################\n",
        "# We have two sets of embeddings:\n",
        "# 1) center_word_embeddings\n",
        "# 2) context_word_embeddings\n",
        "# shape = (vocab_size, EMBED_DIM)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "random.seed(0)\n",
        "def random_init():\n",
        "    # small random range\n",
        "    return (random.random() - 0.5) / EMBED_DIM\n",
        "\n",
        "center_embeddings = [[random_init() for _ in range(EMBED_DIM)] for _ in range(vocab_size)]\n",
        "context_embeddings = [[random_init() for _ in range(EMBED_DIM)] for _ in range(vocab_size)]\n",
        "\n",
        "###############################################################################\n",
        "# 7. TRAINING LOOP (Skip-gram with Negative Sampling)\n",
        "###############################################################################\n",
        "# We'll do a very naive gradient descent approach, single-sample updates,\n",
        "# with no mini-batches. This is purely for demonstration and can be slow.\n",
        "\n",
        "def sigmoid(x):\n",
        "    # stable sigmoid\n",
        "    if x > 6:\n",
        "        return 1.0\n",
        "    elif x < -6:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return 1.0 / (1.0 + math.exp(-x))\n",
        "\n",
        "pairs_count = len(training_pairs)\n",
        "iteration_count = EPOCHS * pairs_count\n",
        "current_lr = LEARNING_RATE\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "\n",
        "step = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    random.shuffle(training_pairs)\n",
        "    for (center_i, context_i) in training_pairs:\n",
        "        # 1) Positive sample\n",
        "        # Vector for center word\n",
        "        center_vec = center_embeddings[center_i]\n",
        "        # Vector for context word\n",
        "        context_vec = context_embeddings[context_i]\n",
        "\n",
        "        # Dot product\n",
        "        dot_pos = sum(cv * cc for cv, cc in zip(center_vec, context_vec))\n",
        "        pred_pos = sigmoid(dot_pos)  # predicted probability that this is a true pair\n",
        "        # Error gradient for positive sample (label=1)\n",
        "        grad_pos = (1 - pred_pos)\n",
        "\n",
        "        # Update center & context embeddings for positive sample\n",
        "        # gradient wrt center = grad_pos * context_vec\n",
        "        # gradient wrt context = grad_pos * center_vec\n",
        "        for d in range(EMBED_DIM):\n",
        "            c_val = center_vec[d]\n",
        "            ctx_val = context_vec[d]\n",
        "\n",
        "            # update center\n",
        "            center_embeddings[center_i][d] += current_lr * grad_pos * ctx_val\n",
        "            # update context\n",
        "            context_embeddings[context_i][d] += current_lr * grad_pos * c_val\n",
        "\n",
        "        # 2) Negative samples\n",
        "        for _ in range(NEGATIVE_SAMPLES):\n",
        "            neg_i = sample_negative()\n",
        "            # if by chance neg_i == center_i or context_i, we still treat it as negative\n",
        "            neg_vec = context_embeddings[neg_i]\n",
        "            dot_neg = sum(cv * nv for cv, nv in zip(center_vec, neg_vec))\n",
        "            pred_neg = sigmoid(dot_neg)  # predicted probability that this is a true pair\n",
        "            # label=0 => error gradient = (0 - pred_neg) = -pred_neg\n",
        "            grad_neg = (0 - pred_neg)\n",
        "\n",
        "            # update center & negative embeddings\n",
        "            for d in range(EMBED_DIM):\n",
        "                c_val = center_embeddings[center_i][d]\n",
        "                n_val = context_embeddings[neg_i][d]\n",
        "\n",
        "                center_embeddings[center_i][d] += current_lr * grad_neg * n_val\n",
        "                context_embeddings[neg_i][d] += current_lr * grad_neg * c_val\n",
        "\n",
        "        step += 1\n",
        "        # Decay learning rate a tiny bit each step\n",
        "        current_lr = LEARNING_RATE * (1.0 / (1.0 + DECAY * step))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} completed.\")\n",
        "\n",
        "print(\"Training done.\\n\")\n",
        "\n",
        "###############################################################################\n",
        "# 8. RESULT: The 'center_embeddings' now hold your word embeddings\n",
        "###############################################################################\n",
        "# Typically, we'd use (center_embeddings + context_embeddings)/2 or just center_embeddings\n",
        "# as the final word vectors.\n",
        "\n",
        "# Let's define a helper dict for convenience:\n",
        "word_vectors = {}\n",
        "for i, w in enumerate(vocab):\n",
        "    # Some approaches average center & context embeddings\n",
        "    merged_vec = [(c + ctx)/2.0 for c, ctx in zip(center_embeddings[i], context_embeddings[i])]\n",
        "    word_vectors[w] = merged_vec\n",
        "\n",
        "print(\"Example embedding for a few words in your vocab:\")\n",
        "sample_words = [\"machine\", \"learning\", \"data\", \"model\", \"neural\"]\n",
        "for sw in sample_words:\n",
        "    if sw in word_vectors:\n",
        "        vec_str = \", \".join(f\"{val:.3f}\" for val in word_vectors[sw][:10])  # show first 10 dims\n",
        "        print(f\"{sw} => [{vec_str}, ... ]\")\n",
        "    else:\n",
        "        print(f\"'{sw}' not in vocab (freq < {MIN_FREQ}?).\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 (2)"
      ],
      "metadata": {
        "id": "1s45mKrPpjoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################################\n",
        "# 1. PRETEND WE ALREADY HAVE A 'word_vectors' DICT\n",
        "###############################################################################\n",
        "# In your actual notebook, you should run your word2vec code first, which ends\n",
        "# with something like:\n",
        "#\n",
        "#   word_vectors = {\n",
        "#       \"word\": [0.23, -0.12, 0.54, ...],  # 300-dim embedding\n",
        "#       ...\n",
        "#   }\n",
        "#\n",
        "# We'll fake a small example here for demonstration. In practice, replace\n",
        "# this entire dictionary with the actual embeddings you computed.\n",
        "word_vectors = {\n",
        "    \"machine\":  [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"learning\": [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"deep\":     [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"neural\":   [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"network\":  [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"data\":     [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"model\":    [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"training\": [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"test\":     [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"result\":   [random.gauss(0, 1) for _ in range(300)],\n",
        "    # ... in reality, you'd have thousands of words here\n",
        "}\n",
        "\n",
        "# If your real word_vectors is huge, consider sub-sampling:\n",
        "# word_vectors_subset = {w: word_vectors[w] for w in some_top_words}\n",
        "# For demonstration, we'll just use the entire small dict.\n",
        "\n",
        "words = list(word_vectors.keys())  # the vocab words\n",
        "vectors = [word_vectors[w] for w in words]  # list of 300-dim vectors\n",
        "\n",
        "###############################################################################\n",
        "# 2. PCA FROM SCRATCH (REDUCE 300D -> 2D)\n",
        "###############################################################################\n",
        "# Steps:\n",
        "#   1) Compute mean of each dimension.\n",
        "#   2) Subtract mean from each vector (center).\n",
        "#   3) Compute covariance matrix = (X^T X) / (N-1), where X is data matrix.\n",
        "#   4) Compute eigenvectors, eigenvalues of covariance matrix.\n",
        "#   5) Sort eigenvectors by descending eigenvalues; pick top 2.\n",
        "#   6) Project all data points to these 2 eigenvectors.\n",
        "\n",
        "def mean_center(vectors_2d):\n",
        "    \"\"\"\n",
        "    vectors_2d: list of (300-dim) lists\n",
        "    Returns:\n",
        "       centered_data (list of lists), means (list of float).\n",
        "    \"\"\"\n",
        "    n = len(vectors_2d)      # number of points\n",
        "    d = len(vectors_2d[0])   # dimension (300)\n",
        "\n",
        "    # Compute mean of each dimension\n",
        "    means = [0.0]*d\n",
        "    for vec in vectors_2d:\n",
        "        for i in range(d):\n",
        "            means[i] += vec[i]\n",
        "    for i in range(d):\n",
        "        means[i] /= float(n)\n",
        "\n",
        "    # Center the data\n",
        "    centered = []\n",
        "    for vec in vectors_2d:\n",
        "        centered.append([vec[i] - means[i] for i in range(d)])\n",
        "    return centered, means\n",
        "\n",
        "def matrix_multiply(A, B):\n",
        "    \"\"\"\n",
        "    Multiply two matrices (lists of lists).\n",
        "    A is n x d, B is d x m => result is n x m.\n",
        "    From scratch, purely for demonstration.\n",
        "    \"\"\"\n",
        "    n = len(A)\n",
        "    d = len(A[0])\n",
        "    # B is d x m\n",
        "    m = len(B[0])\n",
        "\n",
        "    # result is n x m\n",
        "    result = [[0.0]*m for _ in range(n)]\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            s = 0.0\n",
        "            for k in range(d):\n",
        "                s += A[i][k] * B[k][j]\n",
        "            result[i][j] = s\n",
        "    return result\n",
        "\n",
        "def transpose(M):\n",
        "    \"\"\"\n",
        "    Transpose a matrix M (list of lists).\n",
        "    \"\"\"\n",
        "    rows = len(M)\n",
        "    cols = len(M[0])\n",
        "    # create a cols x rows matrix\n",
        "    T = [[0.0]*rows for _ in range(cols)]\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            T[c][r] = M[r][c]\n",
        "    return T\n",
        "\n",
        "def covariance_matrix(X):\n",
        "    \"\"\"\n",
        "    Compute the d x d covariance matrix of X (n x d).\n",
        "    X is already mean-centered.\n",
        "    \"\"\"\n",
        "    n = len(X)          # number of samples\n",
        "    d = len(X[0])       # dimension\n",
        "    # Cov = (X^T X) / (N-1)\n",
        "    # We'll do X^T first, then multiply (X^T * X), scale by 1/(n-1)\n",
        "\n",
        "    X_t = transpose(X)  # d x n\n",
        "    # (X^T X) => d x d\n",
        "    XT_X = matrix_multiply(X_t, X)\n",
        "\n",
        "    # scale by (1/(n-1))\n",
        "    scale = 1.0 / (n - 1)\n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            XT_X[i][j] *= scale\n",
        "    return XT_X  # d x d\n",
        "\n",
        "def eigen_decomposition_symmetric(M):\n",
        "    \"\"\"\n",
        "    Very naive eigen decomposition for a real symmetric matrix M (d x d).\n",
        "    We'll do a simple power iteration or Jacobi. For demonstration only.\n",
        "\n",
        "    Return:\n",
        "      eigenvalues (list), eigenvectors (list of lists)\n",
        "    \"\"\"\n",
        "    # Because we have a 300 x 300 matrix, doing a naive approach can be slow.\n",
        "    # We'll do a simple placeholder or smaller dimension.\n",
        "    # For demonstration, let's do a simplified power iteration for the top 2 eigenvectors.\n",
        "\n",
        "    d = len(M)\n",
        "\n",
        "    def mat_vec_mul(mat, vec):\n",
        "        result = [0.0]*d\n",
        "        for i in range(d):\n",
        "            s = 0.0\n",
        "            for j in range(d):\n",
        "                s += mat[i][j] * vec[j]\n",
        "            result[i] = s\n",
        "        return result\n",
        "\n",
        "    def normalize(vec):\n",
        "        norm = math.sqrt(sum(x*x for x in vec))\n",
        "        if norm < 1e-9:\n",
        "            return vec\n",
        "        return [x/norm for x in vec]\n",
        "\n",
        "    eigenvals = []\n",
        "    eigenvecs = []\n",
        "\n",
        "    # We want 2 eigenvectors (since we only want a 2D projection)\n",
        "    k = 2\n",
        "    used_ortho = []\n",
        "\n",
        "    for _ in range(k):\n",
        "        # pick a random initial vector\n",
        "        v = [random.random() for _ in range(d)]\n",
        "        v = normalize(v)\n",
        "\n",
        "        # power iteration\n",
        "        for __ in range(50):  # 50 iterations\n",
        "            # subtract components of previously found eigenvectors to maintain orthogonality\n",
        "            for u in eigenvecs:\n",
        "                dot_uv = sum(a*b for a,b in zip(u, v))\n",
        "                # subtract\n",
        "                v = [v_i - dot_uv*u_i for v_i, u_i in zip(v, u)]\n",
        "            v = normalize(v)\n",
        "\n",
        "            # multiply by M\n",
        "            v_new = mat_vec_mul(M, v)\n",
        "            # re-orthogonalize\n",
        "            for u in eigenvecs:\n",
        "                dot_uv = sum(a*b for a,b in zip(u, v_new))\n",
        "                v_new = [vn_i - dot_uv*u_i for vn_i, u_i in zip(v_new, u)]\n",
        "            v_new = normalize(v_new)\n",
        "            v = v_new\n",
        "\n",
        "        # approximate eigenvalue = v^T M v\n",
        "        # v is normalized\n",
        "        Mv = mat_vec_mul(M, v)\n",
        "        lam = sum(a*b for a,b in zip(v, Mv))\n",
        "\n",
        "        eigenvals.append(lam)\n",
        "        eigenvecs.append(v)\n",
        "\n",
        "    # pair them and sort by descending eigenvalue\n",
        "    pairs = sorted(zip(eigenvals, eigenvecs), key=lambda x: x[0], reverse=True)\n",
        "    eigenvals, eigenvecs = zip(*pairs)\n",
        "    return list(eigenvals), list(eigenvecs)\n",
        "\n",
        "def project_data(X, evecs):\n",
        "    \"\"\"\n",
        "    Project each row of X (n x d) onto the vectors in 'evecs' (2 vectors, each d-dim).\n",
        "    Return an n x 2 matrix.\n",
        "    \"\"\"\n",
        "    # evecs is [v1, v2], each v ~ d-dim\n",
        "    n = len(X)\n",
        "    d = len(X[0])\n",
        "    k = len(evecs)  # 2\n",
        "    proj = [[0.0]*k for _ in range(n)]\n",
        "    for i in range(n):\n",
        "        for j in range(k):\n",
        "            # dot with evecs[j]\n",
        "            s = 0.0\n",
        "            for c in range(d):\n",
        "                s += X[i][c] * evecs[j][c]\n",
        "            proj[i][j] = s\n",
        "    return proj\n",
        "\n",
        "# --- actually do PCA\n",
        "centered, _ = mean_center(vectors)        # n x 300\n",
        "cov_mat = covariance_matrix(centered)     # 300 x 300\n",
        "# eigen decomposition for top 2\n",
        "evals, evecs = eigen_decomposition_symmetric(cov_mat)  # we get 2 eigenvecs\n",
        "# Project original data\n",
        "pca_2d = project_data(centered, evecs)   # n x 2\n",
        "\n",
        "###############################################################################\n",
        "# 3. PLOT THE PCA RESULT\n",
        "###############################################################################\n",
        "plt.figure(figsize=(6,6))\n",
        "x_vals = [pt[0] for pt in pca_2d]\n",
        "y_vals = [pt[1] for pt in pca_2d]\n",
        "plt.scatter(x_vals, y_vals)\n",
        "\n",
        "# Optionally annotate a few points\n",
        "for i, w in enumerate(words):\n",
        "    plt.text(x_vals[i], y_vals[i], w)\n",
        "\n",
        "plt.title(\"PCA Visualization (2D) of Word Embeddings\")\n",
        "plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# 4. T-SNE FROM SCRATCH (HIGHLY SIMPLIFIED)\n",
        "###############################################################################\n",
        "# Real t-SNE is fairly complicated. We'll do a naive version with:\n",
        "#   1) Compute pairwise similarities in high-d (Gaussian-based).\n",
        "#   2) Initialize low-d points randomly.\n",
        "#   3) Iterative gradient descent to match pairwise similarities in low-d (Student t distribution).\n",
        "# This is for demonstration only, might be SLOW if n is large. We'll only run a few steps.\n",
        "\n",
        "# (a) Build pairwise similarities in high-d\n",
        "def squared_euclid_dist(a, b):\n",
        "    return sum((x-y)*(x-y) for x,y in zip(a,b))\n",
        "\n",
        "def high_dim_affinities(data, perplexity=5.0):\n",
        "    \"\"\"\n",
        "    data is n x d\n",
        "    Return a matrix P of shape (n x n), where P[i][j] is the probability of j given i\n",
        "    under a Gaussian with a certain sigma. We'll do a super naive approach:\n",
        "      - We won't do binary search on perplexity, we'll fix sigma to guess a ballpark.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    # naive guess for sigma\n",
        "    # We'll set sigma = average distance / 2 or something quick\n",
        "    distances = []\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            distances.append(squared_euclid_dist(data[i], data[j]))\n",
        "    avg_dist = sum(distances) / float(len(distances)) if distances else 1.0\n",
        "    sigma = math.sqrt(avg_dist)/2.0  # super naive\n",
        "\n",
        "    two_sigma_sq = 2.0 * sigma * sigma\n",
        "    P = [[0.0]*n for _ in range(n)]\n",
        "    for i in range(n):\n",
        "        sum_exp = 0.0\n",
        "        d_exp = [0.0]*n\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                continue\n",
        "            dist_ij = squared_euclid_dist(data[i], data[j])\n",
        "            val = math.exp(-dist_ij / two_sigma_sq)\n",
        "            d_exp[j] = val\n",
        "            sum_exp += val\n",
        "        for j in range(n):\n",
        "            P[i][j] = d_exp[j] / (sum_exp + 1e-12)\n",
        "    # we then symmetrize P and scale down\n",
        "    # but let's skip complexities, keep it simple\n",
        "    return P\n",
        "\n",
        "# (b) Student-t distribution in low-d\n",
        "def low_dim_affinities(Y):\n",
        "    \"\"\"\n",
        "    Y is n x 2\n",
        "    Return Q[i][j] = (1/(1 + dist^2)) / Z\n",
        "    where dist^2 is squared Euclidian in 2D\n",
        "    \"\"\"\n",
        "    n = len(Y)\n",
        "    Q = [[0.0]*n for _ in range(n)]\n",
        "    sum_q = 0.0\n",
        "    # compute all pairwise\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                Q[i][j] = 0.0\n",
        "            else:\n",
        "                dist_ij = squared_euclid_dist(Y[i], Y[j])\n",
        "                val = 1.0/(1.0 + dist_ij)\n",
        "                Q[i][j] = val\n",
        "                sum_q += val\n",
        "    # normalize\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            Q[i][j] /= sum_q\n",
        "    return Q\n",
        "\n",
        "def tsne_from_scratch(data, perplexity=5.0, max_iter=200, lr=0.1):\n",
        "    \"\"\"\n",
        "    data: n x d\n",
        "    Returns an n x 2 array with the t-SNE embedding.\n",
        "\n",
        "    This is extremely naive and not optimized.\n",
        "    We'll do just a few steps of gradient descent.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return []\n",
        "    # 1) compute P in high dimension\n",
        "    P = high_dim_affinities(data, perplexity=perplexity)\n",
        "    # 2) random init Y in 2D\n",
        "    Y = [[(random.random() - 0.5)*1e-3, (random.random() - 0.5)*1e-3] for _ in range(n)]\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        Q = low_dim_affinities(Y)\n",
        "        # gradient wrt each Y[i]\n",
        "        grads = [[0.0, 0.0] for _ in range(n)]\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                # (P[i][j] - Q[i][j]) * ...\n",
        "                coeff = (P[i][j] - Q[i][j])\n",
        "                # ... times the student t kernel gradient factor\n",
        "                # in 2D: d(distance^2) = 2(Y[i] - Y[j])\n",
        "                # But let's do it carefully:\n",
        "                # dist_ij = Y[i] - Y[j], squared dist = dist_ij^2\n",
        "                dist_x = Y[i][0] - Y[j][0]\n",
        "                dist_y = Y[i][1] - Y[j][1]\n",
        "                dist_sq = dist_x*dist_x + dist_y*dist_y\n",
        "                factor = 1.0/(1.0 + dist_sq)  # from t-SNE gradient\n",
        "                g = 4.0 * factor * coeff\n",
        "\n",
        "                grads[i][0] += g * dist_x\n",
        "                grads[i][1] += g * dist_y\n",
        "\n",
        "        # update Y\n",
        "        for i in range(n):\n",
        "            Y[i][0] += lr * grads[i][0]\n",
        "            Y[i][1] += lr * grads[i][1]\n",
        "\n",
        "        if iteration % 20 == 0:\n",
        "            print(f\"t-SNE iteration {iteration}/{max_iter} done.\")\n",
        "    return Y\n",
        "\n",
        "print(\"\\nRunning naive t-SNE. This may take a while if 'words' is large.\")\n",
        "tsne_2d = tsne_from_scratch(vectors, perplexity=5.0, max_iter=100, lr=0.5)\n",
        "\n",
        "###############################################################################\n",
        "# 5. PLOT THE T-SNE RESULT\n",
        "###############################################################################\n",
        "plt.figure(figsize=(6,6))\n",
        "x_vals = [pt[0] for pt in tsne_2d]\n",
        "y_vals = [pt[1] for pt in tsne_2d]\n",
        "plt.scatter(x_vals, y_vals)\n",
        "\n",
        "# Optionally annotate\n",
        "for i, w in enumerate(words):\n",
        "    plt.text(x_vals[i], y_vals[i], w)\n",
        "\n",
        "plt.title(\"t-SNE Visualization (2D) of Word Embeddings (naive from scratch)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kahk_1gCpm7K",
        "outputId": "f649e785-f0ba-40fb-c179-0b165714194d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running naive t-SNE. This may take a while if 'words' is large.\n",
            "t-SNE iteration 0/100 done.\n",
            "t-SNE iteration 20/100 done.\n",
            "t-SNE iteration 40/100 done.\n",
            "t-SNE iteration 60/100 done.\n",
            "t-SNE iteration 80/100 done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 (3)"
      ],
      "metadata": {
        "id": "hUArL9SBqCiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 1. ASSUME WE ALREADY HAVE A 'word_vectors' DICT\n",
        "###############################################################################\n",
        "# For example, from your Word2Vec training code, at the end you might have:\n",
        "#   word_vectors = {\n",
        "#       \"machine\": [0.23, -0.12, 0.54, ...],  # 300-dim embedding\n",
        "#       ...\n",
        "#   }\n",
        "# Below we'll just artificially create a small dictionary for demonstration.\n",
        "\n",
        "import random\n",
        "\n",
        "word_vectors = {\n",
        "    \"machine\":  [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"learning\": [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"neural\":   [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"network\":  [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"cat\":      [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"dog\":      [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"coffee\":   [random.gauss(0, 1) for _ in range(300)],\n",
        "    \"banana\":   [random.gauss(0, 1) for _ in range(300)],\n",
        "    # ...and so on\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2. DEFINE A COSINE SIMILARITY FUNCTION (FROM SCRATCH)\n",
        "###############################################################################\n",
        "\n",
        "def dot_product(vec1, vec2):\n",
        "    return sum(a*b for a,b in zip(vec1, vec2))\n",
        "\n",
        "def magnitude(vec):\n",
        "    return (sum(x*x for x in vec))**0.5\n",
        "\n",
        "def cosine_similarity(vecA, vecB):\n",
        "    denom = magnitude(vecA) * magnitude(vecB)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return dot_product(vecA, vecB) / denom\n",
        "\n",
        "###############################################################################\n",
        "# 3. CHOOSE SOME WORD PAIRS AND PRINT THEIR SIMILARITIES\n",
        "###############################################################################\n",
        "\n",
        "pairs = [\n",
        "    (\"machine\", \"learning\"),\n",
        "    (\"neural\", \"network\"),\n",
        "    (\"coffee\", \"banana\"),\n",
        "    (\"cat\", \"dog\"),\n",
        "    (\"machine\", \"neural\")\n",
        "]\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    if w1 in word_vectors and w2 in word_vectors:\n",
        "        sim = cosine_similarity(word_vectors[w1], word_vectors[w2])\n",
        "        print(f\"Cosine similarity({w1}, {w2}) = {sim:.4f}\")\n",
        "    else:\n",
        "        print(f\"Either '{w1}' or '{w2}' not in vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKNPGmVEqFM9",
        "outputId": "d7be7c9f-f8dd-4da7-bd33-c9a68def367b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity(machine, learning) = -0.0189\n",
            "Cosine similarity(neural, network) = -0.0422\n",
            "Cosine similarity(coffee, banana) = -0.0672\n",
            "Cosine similarity(cat, dog) = -0.0686\n",
            "Cosine similarity(machine, neural) = 0.0440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Which NLP Task you would like perform on your selected dataset\n",
        "(NER, Summarization, Sentiment Analysis, Text classficication)\n",
        "2.  Explain your labeling Schema you have used and mention those labels\n",
        "\n",
        "3.  You can take AI assistance for labeling the data only.\n",
        "\n"
      ],
      "metadata": {
        "id": "U-JtOozpvZ-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DyK54UY6ompS"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "# Link: https://github.com/Hayatk1909/sentiment-dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment by filling this survey link. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "The task was enjoyable and demanding, in my opinion. It was thrilling to gain practical experience in comprehending linguistic subtleties through sentiment analysis and data annotation. It was a little challenging at first to set up the CSV and manage the files, but it was simple once I figured out how to use GitHub uploads. I appreciated having enough time to explore each step thoroughly, although having a little extra flexibility in the deadline would have helped me dive even deeper into the material. Overall, I really loved learning how to generate a labeled datasetâ€”it's a skill Iâ€™m convinced will be beneficial in future endeavours."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}